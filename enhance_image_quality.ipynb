{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import os\n",
    "import cv2  # OpenCV\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, LeakyReLU, Add, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# load and preprocess a map image dataset\n",
    "\n",
    "dataset_path = \"path/to/dataset\"\n",
    "\n",
    "# Define the scale factor for downscaling (e.g., 4x downscale)\n",
    "scale_factor = 4\n",
    "\n",
    "# Function to load and preprocess the dataset\n",
    "def load_and_preprocess_dataset(dataset_path, scale_factor):\n",
    "    lr_images = []  # Store low-resolution images\n",
    "    hr_images = []  # Store high-resolution images\n",
    "\n",
    "    for filename in os.listdir(dataset_path):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            # Load the image\n",
    "            img = cv2.imread(os.path.join(dataset_path, filename))\n",
    "            \n",
    "            # Resize the image to a smaller size for LR\n",
    "            lr_img = cv2.resize(img, (img.shape[1] // scale_factor, img.shape[0] // scale_factor))\n",
    "\n",
    "            # Ensure the LR image has the same size as the HR image\n",
    "            hr_img = cv2.resize(lr_img, (img.shape[1], img.shape[0]))\n",
    "\n",
    "            # Normalize pixel values to [0, 1]\n",
    "            lr_img = lr_img / 255.0\n",
    "            hr_img = hr_img / 255.0\n",
    "\n",
    "            lr_images.append(lr_img)\n",
    "            hr_images.append(hr_img)\n",
    "\n",
    "    return np.array(lr_images), np.array(hr_images)\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "lr_images, hr_images = load_and_preprocess_dataset(dataset_path, scale_factor)\n",
    "\n",
    "# Now you have your LR and HR image pairs for training your SR-GAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define your generator architecture\n",
    "def generator_model():\n",
    "    input_lr = Input(shape=(None, None, 3))\n",
    "    x = Conv2D(64, 9, padding='same')(input_lr)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "\n",
    "    # Add more Conv2D and BatchNormalization layers as needed\n",
    "\n",
    "    x = Conv2D(3, 9, padding='same')(x)\n",
    "    output_hr = Add()([input_lr, x])\n",
    "    return Model(input_lr, output_hr)\n",
    "\n",
    "# Define your discriminator architecture\n",
    "def discriminator_model():\n",
    "    input_hr = Input(shape=(None, None, 3))\n",
    "    x = Conv2D(64, 3, padding='same')(input_hr)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "\n",
    "    # Add more Conv2D layers and Dense layers as needed\n",
    "\n",
    "    output = Conv2D(1, 3, padding='same')(x)\n",
    "    return Model(input_hr, output)\n",
    "\n",
    "# Create GAN\n",
    "def create_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    input_lr = Input(shape=(None, None, 3))\n",
    "    generated_hr = generator(input_lr)\n",
    "    valid = discriminator(generated_hr)\n",
    "    return Model(input_lr, [generated_hr, valid])\n",
    "\n",
    "# Loss functions\n",
    "mean_squared_error = MeanSquaredError()\n",
    "\n",
    "def content_loss(hr, generated_hr):\n",
    "    return mean_squared_error(hr, generated_hr)\n",
    "\n",
    "def adversarial_loss(valid):\n",
    "    return mean_squared_error(tf.ones_like(valid), valid)\n",
    "\n",
    "\n",
    "# Compile and train the GAN\n",
    "def train_srgan(lr_images, hr_images, epochs, batch_size):\n",
    "    generator = generator_model()\n",
    "    discriminator = discriminator_model()\n",
    "    gan = create_gan(generator, discriminator)\n",
    "\n",
    "    generator.compile(loss=content_loss, optimizer=Adam(learning_rate=1e-4))\n",
    "    discriminator.compile(loss=adversarial_loss, optimizer=Adam(learning_rate=1e-4))\n",
    "    gan.compile(loss=[content_loss, adversarial_loss], loss_weights=[1, 1], optimizer=Adam(learning_rate=1e-4))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(0, len(lr_images), batch_size):\n",
    "            # Sample a batch of LR and HR images\n",
    "            batch_lr = lr_images[batch:batch + batch_size]\n",
    "            batch_hr = hr_images[batch:batch + batch_size]\n",
    "\n",
    "            # Generate HR images from LR images\n",
    "            generated_hr = generator.predict(batch_lr)\n",
    "\n",
    "            # Train the discriminator\n",
    "            discriminator.train_on_batch(batch_hr, np.ones((batch_size, 1)))\n",
    "            discriminator.train_on_batch(generated_hr, np.zeros((batch_size, 1)))\n",
    "\n",
    "            # Train the generator (SR-GAN)\n",
    "            gan.train_on_batch(batch_lr, [batch_hr, np.ones((batch_size, 1)])\n",
    "\n",
    "        # Save the generator model weights periodically\n",
    "\n",
    "# Testing the SR-GAN\n",
    "def super_resolve(lr_image, generator):\n",
    "    return generator.predict(lr_image)\n",
    "\n",
    "# Load a sample LR map image for testing\n",
    "# Use your trained generator to enhance the image\n",
    "# Save the enhanced HR image\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Train your SR-GAN model\n",
    "    train_srgan(lr_images, hr_images, epochs=100, batch_size=16)\n",
    "\n",
    "    # Test the trained model on a sample LR image\n",
    "    enhanced_hr_image = super_resolve(lr_image, generator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Save the enhanced image to disk\n",
    "# Save using PIL \n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Convert the enhanced HR image from NumPy array to PIL image\n",
    "enhanced_hr_image_pil = Image.fromarray((enhanced_hr_image * 255).astype('uint8'))\n",
    "\n",
    "# Define the output file path\n",
    "output_path = \"enhanced_map_image.png\"\n",
    "\n",
    "# Save the enhanced HR image using PIL\n",
    "enhanced_hr_image_pil.save(output_path)\n",
    "\n",
    "print(f\"Enhanced image saved at: {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Save the enhanced image to disk\n",
    "# Save using OpenCV \n",
    "\n",
    "import cv2\n",
    "\n",
    "# Scale the image values back to the range [0, 255] and convert to uint8\n",
    "enhanced_hr_image_cv2 = (enhanced_hr_image * 255).astype('uint8')\n",
    "\n",
    "# Define the output file path\n",
    "output_path = \"enhanced_map_image.png\"\n",
    "\n",
    "# Save the enhanced HR image using OpenCV\n",
    "cv2.imwrite(output_path, cv2.cvtColor(enhanced_hr_image_cv2, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "print(f\"Enhanced image saved at: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
